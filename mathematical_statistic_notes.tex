\documentclass[%
	11pt,
	a4paper,
	utf8,
	%twocolumn
		]{article}	

\usepackage{style_packages/podvoyskiy_article_extended}


\begin{document}
\title{Классические и продвинутые темы теории вероятностей и математической статистики}

\author{\itshape Подвойский А.О.}

\date{}
\maketitle

\thispagestyle{fancy}

Здесь приводятся заметки по некоторым вопросам, касающимся машинного обучения, анализа данных, программирования на языках \texttt{Python}, \texttt{R} и прочим сопряженным вопросам так или иначе, затрагивающим работу с данными.


%\shorttableofcontents{Краткое содержание}{1}


\tableofcontents

\section{Эмпирическая и теоретическая функции распределения}

Построим по выборке $ X_1, X_2, \dots, X_n $ случайную ступенчатую функцию $ \hat{F}_n(x) $, возрастающую скачками величины $ 1/n $ в точках $ X_{(i)} $ ($ i $-ая порядковая статистика). Эта функция называется \emph{эмпирической функцией распределения}. Чтобы задать значения в точках разрывов, формально определим ее так, чтобы она была непрерывна справа
\begin{align*}
	\hat{F}_n(x) = \dfrac{1}{n} \sum_{i = 1}^{n} I_{ \{X_{(i)} \leqslant x\} } = \dfrac{1}{n} \sum_{i = 1}^{n} I_{\{X_i \leqslant x\}}.
\end{align*}

В отличие от эмпирической функции распределения выборки, интегральную функцию $ F(x) $ распределения генеральной совокупности называют \emph{теоретической функцией распределения}.

Различие между эмпирической и теоретической функциями распределения $ F(x) $ состоит в том, что {теоретическая функция} определяет \emph{вероятность} события $ X_i \leqslant x $, а эмпирическая функция $ \hat{F}_n(x) $ определяет \emph{относительную частоту} этого события. Из теоремы Бернулли следует, что относительная частота события $ X_i \leqslant x $, т.е. $ \hat{F}_n(x) $ \emph{стремится по вероятности} к вероятности $ F(x) $ этого события, т.е. $ \hat{F}_n(x) \xrightarrow{\mathbf{P}} F(x) $. Другими словами числа $ \hat{F}_n(x) $ и $ F(x) $ мало отличаются одно от другого \cite[191]{gmurman:1972}.

\section{Неравенства Чебышева}

\emph{Неравенство Маркова} (еще называют неравенством Чебышева) дает грубую оценку вероятности события, состоящего в том, что неотрицательная случайная величина $ X $ с конечным математическим ожиданием $ \mu = \mathbf{E}X $ превысит некоторую положительную детерминированную величину $ a $
\begin{align*}
	\mathbf{P}(| X | \geqslant a) \leqslant \dfrac{ \mathbf{E} | X | }{a}, \quad a > 0.
\end{align*}

\emph{Неравенство Чебышева} (неравенством Чебышева-Бьенеме) дает грубую оценку вероятности события, состоящего в том, что случайная величина $ X $ отклонится от своего конечного среднего $ \mu $ на величину небольшую $ a $ 
\begin{align*}
	\mathbf{P}\big(|\,X - \mu\,| \geqslant a\big) \leqslant \dfrac{\sigma^2}{a^2}, \quad a > 0,
\end{align*}
где $ \sigma^2 $ -- диспрерсия $ X $.

Другими словами неравенство Чебышева-Бьенеме дает грубую верхнюю оценку вероятности выброса центированной случайной величины за положительный порог $ a $.

В качестве следствия получим так назыаемое <<правило трех сигм>>, которое означает, что \emph{вероятность случайной величине отличаться от своего математического ожидания более чем на три среднеквадратических отклонения, мала}.

Разумеется, для каждого распределения величина этой вероятности своя. Можно получить верную для всех распределений с конечной дисперсией оценку сверху для вероятности случайной величине отличаться от своего математического ожидания более чем на три корня из дисперсии
\begin{align*}
	\text{Если} \ \mathbf{E} X^2 < \infty, \text{то} \ \mathbf{P}( | X - \mathbf{E} X | \geqslant 3 \sqrt{ \mathbf{D} X } ) \leqslant \dfrac{1}{9}.
\end{align*}

\emph{Неравенство Высочанского-Петунина} дает оценку вероятности события, состоящего в том, что неотрицательная случайная величина $ X $ с одномодальным распределением, конечными средним $ \mu $ и дисперсией $ \sigma^2 $ не отклониться от своего среднего больше чем на $ \lambda \sigma $
\begin{align*}
	\mathbf{P} \big( |\,X - \mu\,| \geqslant \lambda \sigma \big) \leqslant \dfrac{4}{9 \lambda^2}.
\end{align*}

В приложениях математической статистики используется эвристическое правило $ \lambda = 3 $, что соответствует верхней границе вероятности $ \dfrac{4}{81} \approx 0.04938 $.


\section{Доверительные интервалы}

\emph{Доверительный интервал} -- интервал, покрывающий неизвестный скалярный параметр $ \theta $ с заданной \emph{доверительной вероятностью} $ (1 - \alpha) $
\begin{align*}
	P(\hat{\theta}_1(X_1, X_2, \dots, X_n) < \theta < \hat{\theta}_2(X_1, X_2, \dots, X_n)) \geqslant 1 - \alpha,
\end{align*}
где $ \hat{\theta}_{1,2} $ -- нижняя и верхняя граница доверительного интервала (\emph{случайные величины}), $ \alpha $ -- уровень значимости (она же вероятность ошибки первого рода).

Наиболее часто уровень значимости принимают равным 0.05 или 0.01. Если, например, принят уровень значимости равный 0.05, то означает, что в пяти случаях из ста мы рискуем допустить ошибку первого рода (отвергнуть правильную гипотезу) \cite[284]{gmurman:1972}.

\emph{Границы доверительного интервала} являются \emph{случайными величинами} -- функциями от выборки (или другими словами границы доверительного интервала являются \emph{статистиками}) -- поэтому правильнее говорить не о вероятности попадания $ \theta $ в доверительный интервал, а о вероятности того, что доверительный интервал \underline{покроет} неизвестный параметр $ \theta $ \cite[216]{gmurman:1972}.

\paragraph{Интервалы в нормальной модели} Допустим, что элементы выборки $ X_i $ распределены по закону $ \mathcal{N}(\theta, \sigma^2) $, причем параметр масштаба $ \sigma $ известен, а параметр сдвига $ \theta $ -- нет. Эту модель часто применяют к данным, полученным при независимых измерениях некоторой величины $ \theta $ с помощью прибора (или метода), имеющего известную среднюю погрешность (стандартную ошибку) $ \sigma $.

Если случайная величина $ X $ распределена нормально $ \mathcal{N}(\theta, \sigma^2) $, то выборочная средняя $ \bar{X} $, найденная по независимым наблюдениям, также распределена нормально. Параметры распределения таковы \cite{gmurman:1972}
\begin{align*}
	\mathbf{E}(\bar{X}) = \theta, \sqrt{\mathbf{D}(\bar{X})} = \dfrac{\sigma}{\sqrt{n}} \quad \to \quad \bar{X} \sim \mathcal{N}(\theta, \sigma^2/n).
\end{align*}

Для центрированной и нормированной случайной величины $ \sqrt{n}(\bar{X} - \theta)/\sigma \sim \mathcal{N}(0,1) $ в качестве границ интервала с доверительной вероятности $ 1 - \alpha $ можно взять
\begin{align*}
	\hat{\theta}_1 = \bar{X} - \sigma/\sqrt{n} \, x_{1 - \alpha/2}, \quad \hat{\theta}_2 = \bar{X} + \sigma/\sqrt{n} \, x_{1 - \alpha/2}.
\end{align*}

Таким образом, с вероятностью 0.95 истинное значение параметра сдвига $ \theta $ находится в интервале $ \bar{X} \pm 1.96\, \sigma/\sqrt{n} \approx \bar{X} \pm 2\, \sigma/\sqrt{n} $ (правило двух сигм) \cite[147]{lagutin:2009}.

На практике, если значение $ \sigma $ неизвестно, то его заменяют на \emph{состоятельную оценку} $ \hat{\sigma} = S $, где $ S^2 = \dfrac{1}{2}\sum (X_i - \hat{X})^2 $.

А вот если выборка \emph{маленькая}, про ее параметры ничего неизвестно и объем выборки небольшой ($ n \leqslant 30 $), тогда вместо \emph{нормального распределения} используют \emph{распределение Стьюдента} ($ t $-распределение).

Тогда доверительный интервал будет иметь вид
\begin{align*}
	\Big( \bar{x} - \dfrac{s}{\sqrt{n}} t_{\alpha}(n - 1); \bar{x} + \dfrac{s}{\sqrt{n}} t_{\alpha}(n - 1) \Big),
\end{align*}
где $ t_{\alpha}(n-1) $ -- это квантиль распределения Стьюдента уровня $ 1 - \alpha / 2 $ с $ n - 1 $ степенями свободы.

\remark{
Распределение Стьюдента стремиться к нормальному распределению при $ n \to \infty $
}

\emph{Число степеней свободы} зависит от того, сколько имеется связей между наблюдениями. Так как мы знаем среднее, то наблюдения связаны одним равенством и степеней свободы становится на одну меньше.

Оценка $ \hat{\theta} $ параметра $ \theta $ называется \emph{состоятельной}, если для всех $ \theta \in \Theta $ последовательность
\begin{align*}
	\hat{\theta}_n = \hat{\theta}(X_1, \dots, X_n) \xrightarrow{\mathbf{P}} \theta, \quad n \to \infty.
\end{align*}

Здесь $ \xrightarrow{\mathbf{P}} $ обозначает \emph{сходимость по вероятности}
\begin{align*}
	\forall \varepsilon > 0, \,\, \mathbf{P}( | \hat{\theta} - \theta | > \varepsilon ) \to 0, \quad n \to \infty.
\end{align*}

\emph{Состоятельность} оценки (а точнее -- последовательности оценок $ \{\hat{\theta}_n\} $) означает концентрацию вероятностной массы около истинного значения параметра $ \theta $ с ростом размера выборки $ n $ \cite[75]{lagutin:2009}.

\section{Сходимости}

Из сходимости <<почти наверное>> следует сходимость <<по вероятности>>. А из сходимости <<по вероятности>> следует сходимость <<почти наверное>>.

Случайные величины $ \xi_1, \xi_2, \ldots, \xi_n $ сходятся при $ n \to \infty $ к случайной величине $ \xi $
\begin{itemize}
	\item \emph{Сходимость <<почти наверное>>} (или с вероятностью 1): $ \xi_n \xrightarrow{\text{п.н.}} \xi $, если $ \mathbf{P}\{ \omega: \xi_n(\omega) \to \xi(\omega) \} = 1 $,
	
	\item \emph{в среднем квадратическом}: $ \xi_n \xrightarrow{\text{с.к.}} \xi $, если $ \mathbf{E}(\xi_n - \xi)^2 \to 0 $,
	
	\item \emph{по вероятности}: $ \xi_n \xrightarrow{\mathbf{P}} \xi $, если $ \forall \varepsilon > 0 \ \mathbf{P}(| \xi_n - \xi | > \varepsilon) \to 0 $, 
	
	\item \emph{по распределению}: $ \xi_n \xrightarrow{d} \xi $, если функция распределения $ F_{\xi_n}(x) $ сходится к $ F_{\xi}(x) $ в точках непрерывности последней.
\end{itemize}



\section{Центральная предельная теорема}

Пусть $ X_1,\dots, X_n $ -- независимые одинаково распределенные случайные величины. Положим $ S_n = X_1 + X_2 + \dots + X_n $.

Если $ 0 < \sigma^2 = \mathbf{D}X_1 < \infty $, то
\begin{align*}
	S^{*}_n = \dfrac{ S_n - \mathbf{E}S_n }{ \sqrt{\mathbf{D} S_n} } = \dfrac{S_n - \mu n}{\sigma \sqrt{n}} \xrightarrow{d} Z, \quad n \to \infty,
\end{align*}
где $ Z $ -- стандартная нормальная случайная величина, $ Z \sim \mathcal{N}(0, 1) $.

По центральной предельной теореме среднее значение одинаково распределенных случайных величин стремится к нормальному распределению. Более того верна теорема.

\emph{Теорема}: Если распределение генеральной совокупности имеет конченые математические ожидание и дисперсию, то при $ n \to \infty $ основные выборочные характеристики (среднее, дисперсия, эмпирическая функция распределения) являются нормальными.

\paragraph{Пример} Пусть случайные величины $ Z_1, \dots, Z_k $ распределены по закону $ \mathcal{N}(0, 1) $ и независимы. Тогда распределение случайной величины $ R^2_k = Z_1^2 + \dots + Z_k^2 $ называют распределением $ \chi^2 $ c $ k $ степенями свободы (кратко $ R_k^2 \sim \chi_k^2 $).

Отметим, что каждое слагаемое имеет гамма-распределение с параметрами $ \alpha = \lambda = 1/2 $, т.е. $ Z_i^2 \sim \Gamma(1/2, 1/2) $. 

Поскольку $ R_k^2 $ -- это сумма независимых и одинакового распределенных случайных величин $ Z_i^2 $, то согласно \emph{центральной предельной теореме} имеет место \emph{сходимость по распределению}
\begin{align*}
	(R_k^2 - \mathbf{E}R_k^2) / \sqrt{\mathbf{D}R_k^2} = (R_k^2 - k)/\sqrt{2 k} \xrightarrow{d} Z \sim \mathcal{N}(0, 1), \quad k \to \infty.
\end{align*}

Нормальное приближение является довольно точным уже при $ k > 30 $.


\section{Фактический (достигаемый) уровень значимости}

При проверке статистических гипотез в общем случае задается малое число $ \alpha $ -- вероятность, с которой мы можем позволить себе отвергнуть верную гипотезу (скажем, 0.05). Это число называют \emph{уровнем значимости}.

Исходя из предположения, что гипотеза $ H $ верна, определяется \emph{наименьшее} (самое крайнее левое) значение $ x_{1 - \alpha} $, удовлетворяющее условию
\begin{align*}
	\mathbf{P}(T(X_1, \dots, X_n) \geqslant x_{1 - \alpha} \,|\, H) = \int\limits_{x_{1 - \alpha}}^{+\infty} p_T(x)\,dx \leqslant \alpha.
\end{align*}

Другими словами, вероятность события, состоящего в том, что статистика примет значение большее $ (1 - \alpha) $-квантиля (вероятность маловероятного события) должна быть не больше заранее заданного уровня значимости $ \alpha $.

Если функция распределения статистики $ T $ непрерывна, то $ x_{1 - \alpha} $ является, очевидно, ее $ (1 - \alpha) $-квантилью. Такое $ x_{1 - \alpha} $ называют \emph{критическим значением}: гипотеза $ H $ отвергается, если
$$ t_0 = T(x_1, \dots, x_n) \geqslant x_{1 - \alpha} $$ (произошло маловероятное событие), и принимается -- в противном случае.

При этом величина
$$ \alpha_0 = \mathbf{P}(T(X_1, \dots, X_n) \geqslant t_0 \,|\, H) = \int\limits_{t_0}^{+\infty} p_T(x)\,dx $$
задает \emph{фактический (достигаемый) уровень значимости}. Он равен вероятности того, что статистика $ T $ (измеряющая степень отклонения полученной реализации от наиболее типичной) за счет случайности примет значение $ t_0 $ или даже больше. Другими словами, фактический (достигаемый) уровень значимости оценивает вероятность того, что случайная величина $ T(X_1, \dots, X_n) $ попадет в область $ [\,t_0, +\infty\,) $, где $ t_0 $ -- это значение статистики, найденное по выборке.

Фактический (достигаемый) уровень значимости\footnote{p-value} -- наименьший уровень значимости, на котором проверяемая (нулевая) гипотеза принимается\footnote{Наверное, правильнее говорить \emph{не отвергается}} \cite[161]{lagutin:2009}.

Фактический (достигаемый) уровень значимости -- это вероятность получить значение статистики как в эксперименте или более экстремальное ее значение при условии справедливости нулевой гипотезы.

Подытожив сказанное выше, можно получить следующее правило: если фактический (достигаемый) уровень значимости $ \alpha_0 $ меньше заранее заданного уровня значимости $ \alpha $, то говорят, что данные свидетельствуют против нулевой гипотезы $ H_0 $ в пользу альтернативной и у нас есть основания отвергнуть нулевую гипотезу
\begin{align*}
	\boxed{\text{если} \quad \alpha_0 < \alpha \quad \text{тогда} \quad \bcancel{\cancel{H_0}}}
\end{align*}

{\color{red} Критическое значение $ x_{1 - \alpha} $ допускается интерпретировать как квантиль уровня $ (1 - \alpha) $ только для статистик с непрерывной функцией распределения}

Вычисление фактического (достигаемого) уровня значимости нередко позволяет избежать категоричных (и при этом ошибочных) выводов, сделанных только на основе сравнения наблюдаемого значения статистики $ t_0 $ с критическим значением $ x_{1 - \alpha} $, найденным для формально заданного $ \alpha $.


\section{Теоретические и выборочные квантили}

Пусть $ \alpha \in (0, 1) $. Для \emph{непрерывной} функции распределения $ F $ \emph{теоретической} $ \alpha $-\emph{квантилью} $ x_\alpha $ (или квантилью уровня $ \alpha $) называется решение уравнения $ F(x_\alpha) = \alpha $, т.е. $ x_\alpha = F^{-1}(\alpha) $.

Так же, как и в случае медианы ($ \alpha = 1/2 $) это решение может быть не единственным.

Оценить $ x_\alpha $ можно с помощью порядковой статистики $ X_{([\alpha n] + 1)} $, где $ [\cdot] $ -- обозначает целую часть. Эту оценку называют \emph{выборочной} $ \alpha $-\emph{квантилью}.

\section{Ошибки I и II рода}

\paragraph{Пример} рассмотрим модель $ X_i \sim \mathcal{N}(\theta, \sigma^2) $, где дисперсия известна, а математическое ожидание нет. Для проверки гипотезы $ H_0: \theta = \theta_0 $ можно применить критерий, основанный на статистике $ T(X_1, \cdots, X_n) = \bar{X} $.

Если $ H_0 $ верна, то $ \bar{X} \sim \mathcal{N}(\theta_0, \sigma^2/n) $. Найдем \emph{критическое значение} $ t_\alpha $ из условия
\begin{align*}
	\alpha = \mathbf{P}_{\theta_0} (\bar{X} \geqslant t_\alpha).
\end{align*}

Тогда (центрируем и нормируем случайную величину $ \bar{X} $)
\begin{align*}
	\alpha = \mathbf{P} \Bigg( \dfrac{\sqrt{n} (\bar{X} - \theta_0)}{\sigma} \geqslant \dfrac{\sqrt{n} (t_\alpha - \theta_0)}{\sigma} \Bigg) = 1 - \Phi\Bigg(\dfrac{\sqrt{n} (t_\alpha - \theta_0)}{\sigma}\Bigg), \ \text{так как} \ \dfrac{\sqrt{n} (\bar{X} - \theta_0)}{\sigma} \sim \mathcal{N}(0, 1),
\end{align*}
где $ \Phi(x) $ -- функция распределения закона $ \mathcal{N}(0, 1) $.

Из последнего соотношения получаем \emph{критическое значение}
\begin{align*}
	t_\alpha = \theta_0 + \sigma \, x_{1 - \alpha} / \sqrt{n}.
\end{align*}

Если значение выборочного среднего $ \bar{x} \geqslant t_\alpha$, то гипотеза $ H_0 $ отвергается. Если нулевая гипотеза верна, то неравенство $ \bar{X} \geqslant t_\alpha $ выполняется с вероятностью $ \alpha $. Отвергая в этом случае верную гипотезу $ H_0 $, мы совершаем \emph{ошибку I рода}.

С другой стороны, может оказаться, что на самом деле верна не гипотеза $ H_0 $, а ее альтернатива $ H_1 : \theta = \theta_1$. Если при этом случится, что $ \bar{x} < t_\alpha $, то мы примем ошибочную гипотезу $ H_0 $ вместо $ H_1 $, тем самым допустив \emph{ошибку II рода}.

Найдем вероятность $ \beta $ ошибки II рода для рассматриваемой модели. Когда верна альтернативная гипотеза, выборочное среднее распределено по закону $ \mathcal{N}(\theta_1, \sigma^2/n)$, поэтому
\begin{align*}
	\beta = \mathbf{P}_{\theta_1}(\bar{X} < t_\alpha) = \Phi \Bigg( \dfrac{ \sqrt{n}(t_\alpha - \theta_1) }{\sigma} \Bigg) = \Phi \Bigg( x_{1 - \alpha} - \dfrac{ \sqrt{n}(\theta_1 - \theta_0) }{\sigma} \Bigg).
\end{align*}

Обобщить сказанное выше можно так
\begin{align*}
	\alpha = \mathbf{P}(\,\text{Rej} \, H_0^+\,),\ &\text{ошибка I рода},\\
	\beta = \mathbf{P}(\,\text{Rej} \, H_1^+\,) = \mathbf{P}(\,\neg\text{Rej} \, H_0^-\,),\ &\text{ошибка II рода}.
\end{align*}

Гипотеза $ H_0 $ заключается в том, что $ \theta \in \Theta_0 $, а альтернатива $ H_1 $ -- в том, что $ \theta \in \Theta_1 $. Когда множество $ \Theta_0 (\Theta_1) $ состоит из единственной точки, гипотеза $ H_0 $ (альтернатива $ H_1 $) называется \emph{простой}, иначе -- \emph{сложной}.

\section{Оценка Ходжеса-Лемана}

Оценка Ходжеса-Лемана (HL-оценка) -- это оценка \emph{параметра сдвига} случайной величины.

\emph{\color{blue}Оценка Ходжеса-Лемана} параметра сдвига $ \theta $ определяется в виде \emph{\color{blue}медианы средних Уолша} $ (X_i + X_j) / 2, (1 \leqslant i < j \leqslant n) $, общее число которых равно $ n (n + 1) / 2 $, и записывается в виде \cite{shulenin:hl}
\begin{align}\label{eq:hl}
	HL = \text{med} \ \dfrac{ X_i + X_j }{2}, 1 \leqslant i \leqslant j \leqslant n,
\end{align}
где $ \text{med} $ -- выборочная медиана, $ (X_i + X_j) / 2, \ (i \leqslant j) $ -- средние Уолша, $ \{ X_i \}_{i = 1}^n $ -- последовательность независимых одинаково распределенных случайных величин.

Оценка Ходжеса-Лемана определяется как медиана средних Уолша, т.е. медиана ряда

$ z_1 \leqslant z_2 \leqslant \ldots \leqslant z_{ \frac{n(n+1)}{2} } $, где $ z_k = \dfrac{x_i + x_j}{2}, \ (i < j) $. Следует отметить высокую устойчивость этой оценки к отклонениям от нормальности распределения и засоренности выборки аномальными наблюдениями \cite[\strbook{103}]{kobzar:2012}.

Для нормальной модели ее \emph{абсолютная эффективность} $ \text{АЭ}(HL) = 0.955 $, то есть она проигрывает оптимальному выборочному среднему $ \bar{X} $ менее 5\% в эффективности. Оценка Ходжеса-Лемана является $ B $-робастной, и следовательно, защищена от наличия выбросов в выборке. 

В сравнении с этими характеристиками оценки Ходжеса-Лемана, \emph{выборочное среднее} $ \bar{X} $, являясь \underline{оптимальной} оценкой параметра сдвига $ \theta $ \emph{нормального} распределения, имеет абсолютную эффективность $ \text{АЭ}(\bar{X}) = 1 $, однако она теряет свойства оптимальности {\color{deepred}\itshape даже при небольших отклонениях от нормального распределения} \cite{shulenin:hl}.

При использовании \emph{статистики знаков} процедура Ходжеса-Лемана приводит к оценке $ \hat{\theta} $ параметра $ \theta $ в виде \emph{выборочной медианы} $ \hat{\theta} = \text{med} (X_1, \ldots, X_n) $ \cite[\strbook{153}]{shulenin:nonparam}.

Использование \emph{критерия знаковых рангов Уилкоксона} приводит к \underline{\itshape оценке Ходжеса-Лемана} (медиана средних Уолша) \cite[\strbook{166}, \strbook{167}]{shulenin:nonparam} в виде \eqref{eq:hl}.


А использование \emph{одновыборочного критерия Стьюдента} в контексте процедуру Ходжеса-Лемана приводит к \emph{выборочному среднему} $ \bar{X} $ \cite[\strbook{167}]{shulenin:nonparam}.

\section{Критерий Холлендера-Прошана}

В задачах теории надежности экспоненциальное распределение наработки на отказ $ f(x) = \lambda e^{-\lambda x} $ характеризуется значением параметра $ \lambda = const $, т.е. постоянством интенсивности отказов изделия во времени.

Отсюда следует, что вероятность безотказной работы изделия за время $ \Delta t $ определяется только промежутком времени $ \Delta t $ и не зависит от того, работало изделие раньше или нет.

Другими словами, \emph{вероятность безотказной работы} нового изделия и изделия, проработавшего часть времени, должна быть одинакова. Проверка этого обстоятельства и является целью \emph{критерия Холлендера-Прошана} \cite[295]{kobzar:2012}.

Статистикой Холлендера-Прошана является величина \cite[182]{lagutin:2009}
\begin{align*}
	T_n = \sum_{i > j > k} \psi (X_{(i)}, X_{(j)} + X_{(k)}),
\end{align*}

где
$$
  \psi(a, b) = 
    \begin{cases}
    	1, &\text{если}\ a > b,\\
    	1/2, &\text{если}\ a = b,\\
    	0, &\text{если}\ a < b.
    \end{cases}
$$

Суммирование здесь производится по всем $ n(n-1)(n-2)/6 $ упорядоченным тройками $ (i, j, k) $, для которых $ i > j > k $.

Для достаточно большой выборки можно воспользоваться нормальным приближением (на основании центральной предельной теоремы)
\begin{align*}
	\dfrac{ T_n - \mathbf{E}T_n }{ \sqrt{\mathbf{D}T_n} } \xrightarrow{d} \xi \sim \mathcal{N}(0, 1),
\end{align*}
где
\begin{align*}
	\mathbf{E} T_n = n(n-1)(n-2)/8, \ 
	\mathbf{D} T_n = \dfrac{3}{2}n(n-1)(n-2)\Bigg[ \dfrac{5}{2592}(n - 3)(n - 4) + \dfrac{7}{432}(n - 3) + \dfrac{1}{48} \Bigg].
\end{align*}

\section{Ковариация и корреляция}

Случайные величины $ X $ и $ Y $ называют независимыми, если их совместная плотность $ f_{XY}(x, y) $ факторизуется, то есть для нее выполняется следующее равенство \cite[\strbook{29}]{shulenin:param}
\begin{align*}
	f_{XY}(x, y) = f_X(x) f_Y(y)
\end{align*}

Факт \emph{независимости} случайных величин $ X $, $ Y $  и <<силу>> (или <<тесноту>>) их связи описывают с помощью таких числовых характеристик, как \textbf{ковариация} $ cov(X, Y) $ и \textbf{коэффициент корреляции} $ \rho_{XY} $.

Ковариация (корреляционный момент) двух случайных величин $ X $ и $ Y $ определяется в виде
\begin{align*}
	cov(X, Y) = \mathbf{E}[ (X - \mathbf{E}X)( Y - \mathbf{E}Y) ] = \mathbf{E}(XY) - \mathbf{E}X \mathbf{E}Y,
\end{align*}
где $ \mathbf{E}X $ и $ \mathbf{E}Y $ -- математические ожидания случайных величин $ X $ и $ Y $, а $ \mathbf{E}(XY) $ -- математическое ожидание производения случайных величин $ X $ и $ Y $, вычисляемое по формуле
\begin{align*}
	\mathbf{E}(XY)  = \int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} xy f_{XY}(x, y) dx dy.
\end{align*}

Отсюда следует формула для математического ожидания от произведения в общем случае зависимых СВ
\begin{align*}
	 \mathbf{E}(XY) = \mathbf{E}X \, \mathbf{E}Y + cov(X, Y)
\end{align*}

В частном случае независимых СВ
\begin{align*}
	\mathbf{E}(XY) = \mathbf{E}X \, \mathbf{E}Y
\end{align*}

Отметим, что ковариация случайной величины $ X $ с самой собой равна дисперсии, то есть $ cov(X, X) = \mathbf{D}X $. Отметим также, что если СВ $ X $ и $ Y $ \emph{независимые}, то ковариация $ cov(X, Y) = 0 $ \cite[\strbook{29}]{shulenin:param}. Это следует из того, что для независмых СВ $ f_{XY}(x, y) = f_X(x) f_Y(y) $. Тогда
\begin{multline*}
	 cov(X, Y) = \mathbf{E}(XY) - \mathbf{E}X \mathbf{E}Y = \int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} xy f_{XY}(x, y) dx dy - \mathbf{E}X \mathbf{E}Y =...\\ ...=\underbrace{\int\limits_{-\infty}^{+\infty} x f_{X}(x) dx}_{\mathbf{E}X} \underbrace{\int\limits_{-\infty}^{+\infty} y f_{Y}(y) dy}_{\mathbf{E}Y} - \mathbf{E}X \mathbf{E}Y = 0
\end{multline*}

Коэффициентом корреляции СВ $ X $ и $ Y $ называют число $ \rho_{XY} $, которые вычисляют по формулы
\begin{align*}
	\rho_{X, Y} = \dfrac{ cov(X, Y) }{ \sigma_X \sigma_Y },
\end{align*}
и если $ \rho_{XY} = 0 $, то СВ $ X $ и $ Y $ называют \emph{некорреллированными}.

Отметим, что понятия \emph{независимости} и \emph{некоррелированости} не являются тождественными \cite[\strbook{30}]{shulenin:param}. Из независимости СВ $ X $ и $ Y $, то есть из равенства $ cov(X, Y) = 0 $, следует некоррелированность, то есть $ \rho_{X, Y} = 0$.

Обратное утверждение в общем случае неверно, исключение составляет, например, гауссовский случай, для которого из некоррелированности следует независимость.

Рассмотрим гауссовский случай, то есть предполагаем, что $ L(X, Y) = NN(a_X, a_Y, \sigma_X^2, \sigma_Y^2, \rho) $. Эта запись означает, что двумерная СВ $ Z = (X, Y) $ имеет двумерное нормальное распределение вероятностей с плотностью \cite[\strbook{46}]{shulenin:param}
\begin{multline*}
	f_{XY}(x, y) = \dfrac{1}{ 2 \pi \sigma_X\sigma_Y \sqrt{1 - \rho^2}} \times \ldots\\
	\ldots \times \exp \Bigg( - \dfrac{1}{ 2(1 - \rho^2) }  \bigg[ \bigg( \dfrac{x - a_X}{\sigma_X} \bigg)^2 - 2 \rho \bigg( \dfrac{x - a_X}{\sigma_X} \bigg) \bigg( \dfrac{y - a_Y}{\sigma_Y} \bigg) + \bigg( \dfrac{x - a_Y}{\sigma_Y} \bigg)^2 \bigg] \Bigg),
\end{multline*}
где $ a_x = \mathbf{E}X $, $ a_Y = \mathbf{E}Y $, $ \sigma_X^2 = \mathbf{D}X $, $ \sigma_Y^2 = \mathbf{D}Y $ и $ \rho $ -- коээфициент корреляции.

Пусть СВ $ X $ и $ Y $ некоррелированы, т.е. $ \rho_{X, Y} = 0 $. Тогда выполняется равенство
\begin{align*}
	f_{XY}(x, y) = \dfrac{ 1 }{ \sigma_X \sqrt{2 \pi} } \exp \Bigg( - \dfrac{1}{2} \bigg( \dfrac{x - a_X}{\sigma_X} \bigg)^2 \Bigg) \dfrac{ 1 }{ \sigma_Y \sqrt{2 \pi} } \exp \Bigg( - \dfrac{1}{2} \bigg( \dfrac{y - a_Y}{\sigma_Y} \bigg)^2 \Bigg) = f_X(x) \, f_Y(y),
\end{align*}
из которого следует независимость СВ $ X $ и $ Y $.








% Источники в "Газовой промышленности" нумеруются по мере упоминания 
\begin{thebibliography}{99}\addcontentsline{toc}{section}{Список литературы}
	\bibitem{gmurman:1972}{\emph{Гмурман В.Е.} Теория вероятностей и математическая статистика. -- М.: Высшая школа, 1972.~-- 368~с. }
	
	\bibitem{lagutin:2009}{\emph{Лагутин М.Б.} Наглядная математическая статистика. -- М.: БИНОМ, 2009.~-- 472~с. }
	
	\bibitem{kobzar:2012}{\emph{Кобзарь А.И.} Прикладная математическая статистика. Для инженеров и научных работников. -- М.: ФИЗМАТЛИТ, 2012.~-- 816~с. }
	
	\bibitem{shulenin:param}{\emph{Шуленин В.П.} Математическая статистика. Ч.1. Параметрическая статистика. -- Томск. Изд-во НТЛ, 2012. -- 540 с.}
	
	\bibitem{shulenin:nonparam}{\emph{Шуленин В.П.} Математческая статистика. Ч.2. Непараметрическая статистика. -- Томск. Изд-во НТЛ, 2012. -- 388 с.}
	
	\bibitem{shulenin:hl}{\emph{Шуленин В.П.} Свойства адаптивных оценок Ходжеса-Лемана в асимптотике и при конечных объемах выборки // Вестник Томского Государственного Университета. Управление, вычислительная техника и информатика. -- 2010. -- №2(11). -- С. 96-112}
\end{thebibliography}

\end{document}
